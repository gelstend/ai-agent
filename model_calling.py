"""
æ¨¡å‹è°ƒç”¨ä¸»å‡½æ•°ï¼ˆvLLM OpenAI Compatibleï¼‰
"""
import re
import requests
import time
from typing import Dict, Tuple, Optional
from common_script import read_config

config_path = "./config.txt"
config = read_config(config_path)

# é…ç½®
SERVER_IP = config["jay_zhang_a800_2"]["ip"]
PORT = 8000
BASE_URL = f"http://{SERVER_IP}:{PORT}/v1"

MODEL_NAME = "Qwen3-32B"

DEFAULT_HEADERS = {
    "Content-Type": "application/json"
}

THINK_PATTERN = re.compile(
    r"<think>(.*?)</think>",
    re.DOTALL | re.IGNORECASE
)


# vllmæœåŠ¡å¯åŠ¨æŸ¥è¯¢
def wait_for_service_ready(
    base_url: str,
    max_retry: int = 30,
    interval: int = 2
) -> None:
    """ç­‰å¾… vLLM æœåŠ¡ Ready"""
    for i in range(max_retry):
        try:
            rsp = requests.get(f"{base_url}/models", timeout=5)
            if rsp.status_code == 200:
                print("âœ… vLLM æœåŠ¡ Ready")
                print("ğŸ“¦ å¯ç”¨æ¨¡å‹ï¼š", rsp.json())
                return
        except Exception as e:
            print(f"â³ ç­‰å¾…æœåŠ¡å°±ç»ª ({i + 1}/{max_retry}):", e)

        time.sleep(interval)

    raise RuntimeError("âŒ vLLM æœåŠ¡æœªå¯åŠ¨æˆ–æœªå°±ç»ª")


# æ„å»ºè¯·æ±‚ä½“ï¼Œprompt
def build_payload(
    system_input: str,
    user_input: str,
    model: str,
    # æœ€å¤§çª—å£
    max_tokens: int = 2048*4,
    temperature: float = 0.7,
    top_p: float = 0.9
) -> Dict:
    """æ„é€  Chat Completion è¯·æ±‚ä½“"""
    return {
        "model": model,
        "messages": [
            {"role": "system", "content": system_input},
            {"role": "user", "content": user_input}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p
    }


def parse_response(response_json: Dict) -> Tuple[Optional[str], Optional[str]]:
    """
    è§£ææ¨¡å‹è¿”å›ï¼Œæ”¯æŒï¼š
    1. reasoning_contentï¼ˆQwen / DeepSeekï¼‰
    2. æ˜¾å¼ think å­—æ®µ
    3. <think>...</think> æ–‡æœ¬æ ‡ç­¾
    4. æ™®é€šæ–‡æœ¬å…œåº•
    """

    message = response_json["choices"][0]["message"]

    think = None
    content = message.get("content", "")

    # ===== ç»“æ„åŒ– reasoning_contentï¼ˆæœ€ä¼˜ï¼‰=====
    if "reasoning_content" in message:
        return message.get("reasoning_content"), content

    # ===== æ˜¾å¼ think å­—æ®µ =====
    if "think" in message:
        return message.get("think"), content

    # ===== <think>...</think> æ ‡ç­¾è§£æ =====
    if isinstance(content, str):
        match = THINK_PATTERN.search(content)
        if match:
            think = match.group(1).strip()

            # æŠŠ think ä»æœ€ç»ˆå›ç­”ä¸­ç§»é™¤
            content = THINK_PATTERN.sub("", content).strip()

            return think, content

    # ===== æ™®é€šæ¨¡å‹å…œåº• =====
    return None, content


def chat_completion(payload: Dict) -> Dict:
    """å‘é€ Chat Completion è¯·æ±‚"""
    rsp = requests.post(
        f"{BASE_URL}/chat/completions",
        headers=DEFAULT_HEADERS,
        json=payload,
        timeout=600
    )
    rsp.raise_for_status()
    return rsp.json()


def main():
    system_input = "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„å°æœºå™¨äººï¼Œå«233ï¼Œè€å¿ƒï¼Œè°¨æ…çš„å›ç­”ç”¨æˆ·çš„é—®é¢˜~"
    user_input = "ä½ å¥½ï¼Œç®€å•åšä¸ªè‡ªæˆ‘ä»‹ç»"

    wait_for_service_ready(BASE_URL)

    payload = build_payload(
        system_input=system_input,
        user_input=user_input,
        model=MODEL_NAME
    )

    print("\nğŸ“¨ å‘é€è¯·æ±‚")
    print("ğŸ‘¤ User:", user_input)

    response_json = chat_completion(payload)

    think, context = parse_response(response_json)
    # æ‰“å°æ¨¡å‹æ€è€ƒ
    if think:
        print("\nğŸ§  Model Think:")
        print(think)

    # æ‰“å°æ¨¡å‹è¾“å‡º
    print("\nğŸ¤– Model Answer:")
    print(context)


if __name__ == "__main__":
    main()
